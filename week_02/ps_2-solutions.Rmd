---
title: Week 2 Solutions
author: Gov January Linear Algebra Review
date: 2021-01-11
output: tint::tintPdf
header-includes:
  - \usepackage{bm}
  - \usepackage{mathtools}
---


1. (Strang 3.1.19) Describe the column spaces (lines or planes) of these particular matrices:

\[
A = \begin{bmatrix} 1 & 2 \\ 0 & 0 \\ 0 & 0 \end{bmatrix} \quad \text{ and } \quad
B = \begin{bmatrix} 1 & 0 \\ 0 & 2 \\ 0 & 0 \end{bmatrix} \quad \text{ and } \quad
C = \begin{bmatrix} 1 & 0 \\ 2 & 0 \\ 0 & 0 \end{bmatrix}
\]
\color{red}

The column space of $A$ is the $x$-axis or all vectors $(x, 0, 0)$ \textit{or} a line, since the second column is dependent on the first (there's only one truly informative column).

The column space of $B$ is the $xy$-plane, the possible linear combinations of the two independent column vectors we can produce from row reduction: $\begin{bmatrix} 1  \\ 0 \\ 0 \end{bmatrix}$ and $\begin{bmatrix} 0  \\ 1 \\ 0 \end{bmatrix}$. 

The column space of $C$ is the line of vectors $(x, 2x, 0)$. This is apparent if you look at the system of equations generated by $C^{T} = \begin{bmatrix} 1 & 2 & 0 \\ 0 & 0 & 0 \end{bmatrix}$ and the possible solutions according to Gauss-Jordan elimination.

\color{black}

2. (Strang 3.1.23) If we add an extra column \(\bm{b}\) to a matrix \(A\), then the column space gets larger unless \underline{\phantom{this is some space}}. Give an example where the column space gets larger and an example where it doesn't. Why is \(A\bm{x} = \bm{b}\) solvable exactly when the column space *doesn't* get larger---it is the same for \(A\) and \([A\;\;\bm{b}]\).

\color{red}

The extra column $b$ enlarges the column space unless $b$ is \textbf{already in the column space}, or when the column space *doesn't* get larger---it is the same for \(A\) and \([A\;\;\bm{b}]\).

It is solvable because if the column space doesn't get larger, it means that \textit{$b$ is reachable via linear combinations of the column vectors of $A$.} -- ergo, there is a unique solution!

\color{black}

3. (Strang 3.2.22) If \(AB = 0\) then the column space of \(B\) is contained in the \underline{\phantom{hello there}} of \(A\). Why?

\color{red}

If $A$ times every column of $B$ is zero -- i.e., $A \cdot B_1 = 0,\ldots,A \cdot B_n = 0$ -- then the column space of $B$ -- $\mathbf{C}(B)$ -- is contained in the \textbf{nullspace} of $A$. An example is $A = \begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix}$ and $B = \begin{bmatrix} 1 & 1 \\ -1 & -1 \end{bmatrix}$. Here $\mathbf{C}(B)$ equals $\mathbf{N}(A)$. 

\color{black}


4. (Strang 3.2.39) Fill out these matrices so that they have rank 1:

\[
A = \begin{bmatrix} 1 & 2 & 4 \\ 2 &  & \\ 4 &  &  \end{bmatrix} \quad \text{ and } \quad
B = \begin{bmatrix} & 9 &  \\ 1 & & \\ 2 & 6 & -3 \end{bmatrix} \quad \text{ and } \quad
M = \begin{bmatrix} a & b \\ c &   \end{bmatrix}
\]

\color{red}

Things that are true if these matrices have rank 1:

* There are no independent columns \textit{or} rows.
* There is only one pivot after Gauss-Jordan elimination.

How do you \underline{make} a matrix with rank 1? You could place variables, e.g. $a$, $b$, in the blank spaces and then perform Gauss-Jordan elimination and then determine what values would make the matrix only have one pivot. Or you could "eyeball" it.

For $A$ (1) the first column is the same as the first row and (2) the second and third entries are multiple of the first, so we can make each row/column the same multiple of the first row/column.

For $B$, the third row is complete -- if we just fill in the second row to be a multiple of this third row and then be careful to make sure our fill-ins for the first row \textit{do not} produce multiples of the other rows and columns, we're golden.

For $M$, we could turn the second row in a $c/a$ multiple of the first row -- the first entry in the second row is already a $c/a$ multiple of $a$, and $\mathbf{bc/a}$ makes the second entry a multiple.

$$
A = \begin{bmatrix} 1 & 2 & 4 \\ 2 & \mathbf{4} & \mathbf{8} \\ 4 & \mathbf{8} & \mathbf{16}  \end{bmatrix} \quad \text{ and } \quad
B = \begin{bmatrix} \mathbf{2} & 9 & \mathbf{-3} \\ 1 & \mathbf{3} & \mathbf{-3/2} \\ 2 & 6 & -3 \end{bmatrix} \quad \text{ and } \quad
M = \begin{bmatrix} a & b \\ c &  \mathbf{bc/a} \end{bmatrix}
$$


\color{black}


5. (Strang 3.3.19) Find the rank of \(A\) and also the rank of \(A^{T}A\) and also the rank of \(AA^{T}\):

\[
A = \begin{bmatrix} 1 & 1 & 5 \\ 1 & 0 & 1\end{bmatrix} \quad \text{and} \quad
A = \begin{bmatrix} 2 & 0 \\ 1 & 1 \\ 1 & 2 \end{bmatrix}
\]

\color{red}

Both matrices $A$ have rank 2: an easy way to tell is there are 2 pivots in each (see Strang page 155 for a nice summary).

$A^{T}A$ and $AA^{T}$ will always have the same rank as $A$. Intuitively, this is because we can't ``create more unique data'' by just multiplying a matrix by itself. 

Technically, we can see this by writing out the individual columns of the result of $A^{T}A$ for any matrix $A$:

$$
A^{T}A = \begin{bmatrix} A^{T} \underbrace{a^{\prime}_1}_{ \text{first column of A} } & \cdots & A^{T} a^{\prime}_n \end{bmatrix}
$$

Each column is of the form $A^{T} a^{\prime}_i$, which by definition must lie in the column space of $A^{T}$. Therefore, all columns of $A^{T}A$ must be in the column space of $A^{T}$; since they share the same column space, they share the same rank. The same argument applies to $AA^{T}$. 

\color{black}

6. (Strang 3.4.2) Find the largest possible number of independent vectors among:
\[
\bm{v}_1 = \begin{bmatrix} 1 \\ -1\\ 0 \\ 0 \end{bmatrix}
\bm{v}_2 = \begin{bmatrix} 1 \\ 0 \\ -1 \\ 0 \end{bmatrix}
\bm{v}_3 = \begin{bmatrix} 1 \\ 0 \\ 0 \\ -1 \end{bmatrix}
\bm{v}_4 = \begin{bmatrix}  0  \\ 1 \\ -1\\ 0 \end{bmatrix}
\bm{v}_5 = \begin{bmatrix}  0  \\ 1 \\ 0 \\ -1 \end{bmatrix}
\bm{v}_6 = \begin{bmatrix}  0  \\ 0 \\ 1 \\ -1 \end{bmatrix}
\]

\color{red}

$v_1,v_2,v_3$ are independent. The $-1$'s are in different positions, so they each present unique but "incomplete" ways to navigate $R^{4}$, only \textbf{spanning} it when we consider them all together (i.e. they form a \textbf{basis}). The rest can all be created using different combinations of the first three rows.

\color{black}

7. (Strang 3.4.7) If \(\bm{w}_1, \bm{w}_2, \bm{w}_3\) are independent vectors, show that the differences \(\bm{v}_1 = \bm{w}_2 - \bm{w}_3\) and \(\bm{v}_2 = \bm{w}_1 - \bm{w}_3\) and \(\bm{v}_3 = \bm{w}_1 - \bm{w}_2\) are *dependent*. Find a combination of the \(\bm{v}\)'s that gives zero. 

\color{red}

After some intense eyeballing, you can garner that the sum $v_1 - v_2 + v_3 = 0$. This means that there is a linear combination of these vectors that sums up to $0$ where the weights are \textit{not} all zero -- recalling one of our definitions of linear independence, they are dependent!

\color{black}
